{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive_Bayes_Classifier_BIKO_MWANGI",
      "provenance": [],
      "authorship_tag": "ABX9TyMx7yX0GR7VUSTzkuIPubCv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bikomwangi/WK_9_IP/blob/main/Naive_Bayes_Classifier_BIKO_MWANGI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WImvlY9UDupz"
      },
      "source": [
        "# Importing Relavant Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAJUEYOa5xZo"
      },
      "source": [
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats.stats import pearsonr\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "\n",
        "%matplotlib inline\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4itBWjIHsAl"
      },
      "source": [
        "# Loading, and exploring the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-_Tmwz_DlfD",
        "outputId": "d7e20d0a-cf1e-4706-a2ec-b80366ff70eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "df = pd.read_csv('/content/spambase.data')\n",
        "# overview of the data\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>0.64</th>\n",
              "      <th>0.64.1</th>\n",
              "      <th>0.1</th>\n",
              "      <th>0.32</th>\n",
              "      <th>0.2</th>\n",
              "      <th>0.3</th>\n",
              "      <th>0.4</th>\n",
              "      <th>0.5</th>\n",
              "      <th>0.6</th>\n",
              "      <th>0.7</th>\n",
              "      <th>0.64.2</th>\n",
              "      <th>0.8</th>\n",
              "      <th>0.9</th>\n",
              "      <th>0.10</th>\n",
              "      <th>0.32.1</th>\n",
              "      <th>0.11</th>\n",
              "      <th>1.29</th>\n",
              "      <th>1.93</th>\n",
              "      <th>0.12</th>\n",
              "      <th>0.96</th>\n",
              "      <th>0.13</th>\n",
              "      <th>0.14</th>\n",
              "      <th>0.15</th>\n",
              "      <th>0.16</th>\n",
              "      <th>0.17</th>\n",
              "      <th>0.18</th>\n",
              "      <th>0.19</th>\n",
              "      <th>0.20</th>\n",
              "      <th>0.21</th>\n",
              "      <th>0.22</th>\n",
              "      <th>0.23</th>\n",
              "      <th>0.24</th>\n",
              "      <th>0.25</th>\n",
              "      <th>0.26</th>\n",
              "      <th>0.27</th>\n",
              "      <th>0.28</th>\n",
              "      <th>0.29</th>\n",
              "      <th>0.30</th>\n",
              "      <th>0.31</th>\n",
              "      <th>0.32.2</th>\n",
              "      <th>0.33</th>\n",
              "      <th>0.34</th>\n",
              "      <th>0.35</th>\n",
              "      <th>0.36</th>\n",
              "      <th>0.37</th>\n",
              "      <th>0.38</th>\n",
              "      <th>0.39</th>\n",
              "      <th>0.40</th>\n",
              "      <th>0.41</th>\n",
              "      <th>0.42</th>\n",
              "      <th>0.778</th>\n",
              "      <th>0.43</th>\n",
              "      <th>0.44</th>\n",
              "      <th>3.756</th>\n",
              "      <th>61</th>\n",
              "      <th>278</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.28</td>\n",
              "      <td>3.47</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.048</td>\n",
              "      <td>5.114</td>\n",
              "      <td>101</td>\n",
              "      <td>1028</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.75</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.36</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.16</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.010</td>\n",
              "      <td>9.821</td>\n",
              "      <td>485</td>\n",
              "      <td>2259</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.223</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>15</td>\n",
              "      <td>54</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0  0.64  0.64.1  0.1  0.32   0.2  ...   0.43   0.44  3.756   61   278  1\n",
              "0  0.21  0.28    0.50  0.0  0.14  0.28  ...  0.180  0.048  5.114  101  1028  1\n",
              "1  0.06  0.00    0.71  0.0  1.23  0.19  ...  0.184  0.010  9.821  485  2259  1\n",
              "2  0.00  0.00    0.00  0.0  0.63  0.00  ...  0.000  0.000  3.537   40   191  1\n",
              "3  0.00  0.00    0.00  0.0  0.63  0.00  ...  0.000  0.000  3.537   40   191  1\n",
              "4  0.00  0.00    0.00  0.0  1.85  0.00  ...  0.000  0.000  3.000   15    54  1\n",
              "\n",
              "[5 rows x 58 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vkli9O5Ej8n",
        "outputId": "25dc010a-bd43-41ac-a025-a8b9260076ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "source": [
        "# checking the statistical distribution\n",
        "df.describe()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>0.64</th>\n",
              "      <th>0.64.1</th>\n",
              "      <th>0.1</th>\n",
              "      <th>0.32</th>\n",
              "      <th>0.2</th>\n",
              "      <th>0.3</th>\n",
              "      <th>0.4</th>\n",
              "      <th>0.5</th>\n",
              "      <th>0.6</th>\n",
              "      <th>0.7</th>\n",
              "      <th>0.64.2</th>\n",
              "      <th>0.8</th>\n",
              "      <th>0.9</th>\n",
              "      <th>0.10</th>\n",
              "      <th>0.32.1</th>\n",
              "      <th>0.11</th>\n",
              "      <th>1.29</th>\n",
              "      <th>1.93</th>\n",
              "      <th>0.12</th>\n",
              "      <th>0.96</th>\n",
              "      <th>0.13</th>\n",
              "      <th>0.14</th>\n",
              "      <th>0.15</th>\n",
              "      <th>0.16</th>\n",
              "      <th>0.17</th>\n",
              "      <th>0.18</th>\n",
              "      <th>0.19</th>\n",
              "      <th>0.20</th>\n",
              "      <th>0.21</th>\n",
              "      <th>0.22</th>\n",
              "      <th>0.23</th>\n",
              "      <th>0.24</th>\n",
              "      <th>0.25</th>\n",
              "      <th>0.26</th>\n",
              "      <th>0.27</th>\n",
              "      <th>0.28</th>\n",
              "      <th>0.29</th>\n",
              "      <th>0.30</th>\n",
              "      <th>0.31</th>\n",
              "      <th>0.32.2</th>\n",
              "      <th>0.33</th>\n",
              "      <th>0.34</th>\n",
              "      <th>0.35</th>\n",
              "      <th>0.36</th>\n",
              "      <th>0.37</th>\n",
              "      <th>0.38</th>\n",
              "      <th>0.39</th>\n",
              "      <th>0.40</th>\n",
              "      <th>0.41</th>\n",
              "      <th>0.42</th>\n",
              "      <th>0.778</th>\n",
              "      <th>0.43</th>\n",
              "      <th>0.44</th>\n",
              "      <th>3.756</th>\n",
              "      <th>61</th>\n",
              "      <th>278</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.104576</td>\n",
              "      <td>0.212922</td>\n",
              "      <td>0.280578</td>\n",
              "      <td>0.065439</td>\n",
              "      <td>0.312222</td>\n",
              "      <td>0.095922</td>\n",
              "      <td>0.114233</td>\n",
              "      <td>0.105317</td>\n",
              "      <td>0.090087</td>\n",
              "      <td>0.239465</td>\n",
              "      <td>0.059837</td>\n",
              "      <td>0.541680</td>\n",
              "      <td>0.093950</td>\n",
              "      <td>0.058639</td>\n",
              "      <td>0.049215</td>\n",
              "      <td>0.248833</td>\n",
              "      <td>0.142617</td>\n",
              "      <td>0.184504</td>\n",
              "      <td>1.662041</td>\n",
              "      <td>0.085596</td>\n",
              "      <td>0.809728</td>\n",
              "      <td>0.121228</td>\n",
              "      <td>0.101667</td>\n",
              "      <td>0.094289</td>\n",
              "      <td>0.549624</td>\n",
              "      <td>0.265441</td>\n",
              "      <td>0.767472</td>\n",
              "      <td>0.124872</td>\n",
              "      <td>0.098937</td>\n",
              "      <td>0.102874</td>\n",
              "      <td>0.064767</td>\n",
              "      <td>0.047059</td>\n",
              "      <td>0.097250</td>\n",
              "      <td>0.047846</td>\n",
              "      <td>0.105435</td>\n",
              "      <td>0.097498</td>\n",
              "      <td>0.136983</td>\n",
              "      <td>0.013204</td>\n",
              "      <td>0.078646</td>\n",
              "      <td>0.064848</td>\n",
              "      <td>0.043676</td>\n",
              "      <td>0.132367</td>\n",
              "      <td>0.046109</td>\n",
              "      <td>0.079213</td>\n",
              "      <td>0.301289</td>\n",
              "      <td>0.179863</td>\n",
              "      <td>0.005446</td>\n",
              "      <td>0.031876</td>\n",
              "      <td>0.038583</td>\n",
              "      <td>0.139061</td>\n",
              "      <td>0.016980</td>\n",
              "      <td>0.268960</td>\n",
              "      <td>0.075827</td>\n",
              "      <td>0.044248</td>\n",
              "      <td>5.191827</td>\n",
              "      <td>52.170870</td>\n",
              "      <td>283.290435</td>\n",
              "      <td>0.393913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.305387</td>\n",
              "      <td>1.290700</td>\n",
              "      <td>0.504170</td>\n",
              "      <td>1.395303</td>\n",
              "      <td>0.672586</td>\n",
              "      <td>0.273850</td>\n",
              "      <td>0.391480</td>\n",
              "      <td>0.401112</td>\n",
              "      <td>0.278643</td>\n",
              "      <td>0.644816</td>\n",
              "      <td>0.201565</td>\n",
              "      <td>0.861791</td>\n",
              "      <td>0.301065</td>\n",
              "      <td>0.335219</td>\n",
              "      <td>0.258871</td>\n",
              "      <td>0.825881</td>\n",
              "      <td>0.444099</td>\n",
              "      <td>0.530930</td>\n",
              "      <td>1.775669</td>\n",
              "      <td>0.509821</td>\n",
              "      <td>1.200938</td>\n",
              "      <td>1.025866</td>\n",
              "      <td>0.350321</td>\n",
              "      <td>0.442681</td>\n",
              "      <td>1.671511</td>\n",
              "      <td>0.887043</td>\n",
              "      <td>3.367639</td>\n",
              "      <td>0.538631</td>\n",
              "      <td>0.593389</td>\n",
              "      <td>0.456729</td>\n",
              "      <td>0.403435</td>\n",
              "      <td>0.328594</td>\n",
              "      <td>0.555966</td>\n",
              "      <td>0.329480</td>\n",
              "      <td>0.532315</td>\n",
              "      <td>0.402664</td>\n",
              "      <td>0.423493</td>\n",
              "      <td>0.220675</td>\n",
              "      <td>0.434718</td>\n",
              "      <td>0.349953</td>\n",
              "      <td>0.361243</td>\n",
              "      <td>0.766900</td>\n",
              "      <td>0.223835</td>\n",
              "      <td>0.622042</td>\n",
              "      <td>1.011787</td>\n",
              "      <td>0.911214</td>\n",
              "      <td>0.076283</td>\n",
              "      <td>0.285765</td>\n",
              "      <td>0.243497</td>\n",
              "      <td>0.270377</td>\n",
              "      <td>0.109406</td>\n",
              "      <td>0.815726</td>\n",
              "      <td>0.245906</td>\n",
              "      <td>0.429388</td>\n",
              "      <td>31.732891</td>\n",
              "      <td>194.912453</td>\n",
              "      <td>606.413764</td>\n",
              "      <td>0.488669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.588000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.310000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.220000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.065000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.275500</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>95.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.420000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.382500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.640000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.270000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.110000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.188000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.314250</td>\n",
              "      <td>0.052000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.705250</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>265.250000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4.540000</td>\n",
              "      <td>14.280000</td>\n",
              "      <td>5.100000</td>\n",
              "      <td>42.810000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>5.880000</td>\n",
              "      <td>7.270000</td>\n",
              "      <td>11.110000</td>\n",
              "      <td>5.260000</td>\n",
              "      <td>18.180000</td>\n",
              "      <td>2.610000</td>\n",
              "      <td>9.670000</td>\n",
              "      <td>5.550000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>4.410000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>7.140000</td>\n",
              "      <td>9.090000</td>\n",
              "      <td>18.750000</td>\n",
              "      <td>18.180000</td>\n",
              "      <td>11.110000</td>\n",
              "      <td>17.100000</td>\n",
              "      <td>5.450000</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>20.830000</td>\n",
              "      <td>16.660000</td>\n",
              "      <td>33.330000</td>\n",
              "      <td>9.090000</td>\n",
              "      <td>14.280000</td>\n",
              "      <td>5.880000</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>4.760000</td>\n",
              "      <td>18.180000</td>\n",
              "      <td>4.760000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>7.690000</td>\n",
              "      <td>6.890000</td>\n",
              "      <td>8.330000</td>\n",
              "      <td>11.110000</td>\n",
              "      <td>4.760000</td>\n",
              "      <td>7.140000</td>\n",
              "      <td>14.280000</td>\n",
              "      <td>3.570000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>21.420000</td>\n",
              "      <td>22.050000</td>\n",
              "      <td>2.170000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>4.385000</td>\n",
              "      <td>9.752000</td>\n",
              "      <td>4.081000</td>\n",
              "      <td>32.478000</td>\n",
              "      <td>6.003000</td>\n",
              "      <td>19.829000</td>\n",
              "      <td>1102.500000</td>\n",
              "      <td>9989.000000</td>\n",
              "      <td>15841.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0         0.64  ...           278            1\n",
              "count  4600.000000  4600.000000  ...   4600.000000  4600.000000\n",
              "mean      0.104576     0.212922  ...    283.290435     0.393913\n",
              "std       0.305387     1.290700  ...    606.413764     0.488669\n",
              "min       0.000000     0.000000  ...      1.000000     0.000000\n",
              "25%       0.000000     0.000000  ...     35.000000     0.000000\n",
              "50%       0.000000     0.000000  ...     95.000000     0.000000\n",
              "75%       0.000000     0.000000  ...    265.250000     1.000000\n",
              "max       4.540000    14.280000  ...  15841.000000     1.000000\n",
              "\n",
              "[8 rows x 58 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv6hOiWJEkF_",
        "outputId": "e9ae54a2-813d-4c7b-9ad3-a85a3ee85045",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# checking the total number of records\n",
        "df.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4600, 58)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6XNWZ8UEkPE",
        "outputId": "1797f1a4-53cd-4fe4-f121-62b994c919e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# checking the data types of the attributes\n",
        "df.info()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4600 entries, 0 to 4599\n",
            "Data columns (total 58 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       4600 non-null   float64\n",
            " 1   0.64    4600 non-null   float64\n",
            " 2   0.64.1  4600 non-null   float64\n",
            " 3   0.1     4600 non-null   float64\n",
            " 4   0.32    4600 non-null   float64\n",
            " 5   0.2     4600 non-null   float64\n",
            " 6   0.3     4600 non-null   float64\n",
            " 7   0.4     4600 non-null   float64\n",
            " 8   0.5     4600 non-null   float64\n",
            " 9   0.6     4600 non-null   float64\n",
            " 10  0.7     4600 non-null   float64\n",
            " 11  0.64.2  4600 non-null   float64\n",
            " 12  0.8     4600 non-null   float64\n",
            " 13  0.9     4600 non-null   float64\n",
            " 14  0.10    4600 non-null   float64\n",
            " 15  0.32.1  4600 non-null   float64\n",
            " 16  0.11    4600 non-null   float64\n",
            " 17  1.29    4600 non-null   float64\n",
            " 18  1.93    4600 non-null   float64\n",
            " 19  0.12    4600 non-null   float64\n",
            " 20  0.96    4600 non-null   float64\n",
            " 21  0.13    4600 non-null   float64\n",
            " 22  0.14    4600 non-null   float64\n",
            " 23  0.15    4600 non-null   float64\n",
            " 24  0.16    4600 non-null   float64\n",
            " 25  0.17    4600 non-null   float64\n",
            " 26  0.18    4600 non-null   float64\n",
            " 27  0.19    4600 non-null   float64\n",
            " 28  0.20    4600 non-null   float64\n",
            " 29  0.21    4600 non-null   float64\n",
            " 30  0.22    4600 non-null   float64\n",
            " 31  0.23    4600 non-null   float64\n",
            " 32  0.24    4600 non-null   float64\n",
            " 33  0.25    4600 non-null   float64\n",
            " 34  0.26    4600 non-null   float64\n",
            " 35  0.27    4600 non-null   float64\n",
            " 36  0.28    4600 non-null   float64\n",
            " 37  0.29    4600 non-null   float64\n",
            " 38  0.30    4600 non-null   float64\n",
            " 39  0.31    4600 non-null   float64\n",
            " 40  0.32.2  4600 non-null   float64\n",
            " 41  0.33    4600 non-null   float64\n",
            " 42  0.34    4600 non-null   float64\n",
            " 43  0.35    4600 non-null   float64\n",
            " 44  0.36    4600 non-null   float64\n",
            " 45  0.37    4600 non-null   float64\n",
            " 46  0.38    4600 non-null   float64\n",
            " 47  0.39    4600 non-null   float64\n",
            " 48  0.40    4600 non-null   float64\n",
            " 49  0.41    4600 non-null   float64\n",
            " 50  0.42    4600 non-null   float64\n",
            " 51  0.778   4600 non-null   float64\n",
            " 52  0.43    4600 non-null   float64\n",
            " 53  0.44    4600 non-null   float64\n",
            " 54  3.756   4600 non-null   float64\n",
            " 55  61      4600 non-null   int64  \n",
            " 56  278     4600 non-null   int64  \n",
            " 57  1       4600 non-null   int64  \n",
            "dtypes: float64(55), int64(3)\n",
            "memory usage: 2.0 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inj4mqzoGL8C",
        "outputId": "e90e54c4-6bd0-4102-b0ba-8718c177a917",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Checking number of null values\n",
        "#\n",
        "df.isnull().sum()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         0\n",
              "0.64      0\n",
              "0.64.1    0\n",
              "0.1       0\n",
              "0.32      0\n",
              "0.2       0\n",
              "0.3       0\n",
              "0.4       0\n",
              "0.5       0\n",
              "0.6       0\n",
              "0.7       0\n",
              "0.64.2    0\n",
              "0.8       0\n",
              "0.9       0\n",
              "0.10      0\n",
              "0.32.1    0\n",
              "0.11      0\n",
              "1.29      0\n",
              "1.93      0\n",
              "0.12      0\n",
              "0.96      0\n",
              "0.13      0\n",
              "0.14      0\n",
              "0.15      0\n",
              "0.16      0\n",
              "0.17      0\n",
              "0.18      0\n",
              "0.19      0\n",
              "0.20      0\n",
              "0.21      0\n",
              "0.22      0\n",
              "0.23      0\n",
              "0.24      0\n",
              "0.25      0\n",
              "0.26      0\n",
              "0.27      0\n",
              "0.28      0\n",
              "0.29      0\n",
              "0.30      0\n",
              "0.31      0\n",
              "0.32.2    0\n",
              "0.33      0\n",
              "0.34      0\n",
              "0.35      0\n",
              "0.36      0\n",
              "0.37      0\n",
              "0.38      0\n",
              "0.39      0\n",
              "0.40      0\n",
              "0.41      0\n",
              "0.42      0\n",
              "0.778     0\n",
              "0.43      0\n",
              "0.44      0\n",
              "3.756     0\n",
              "61        0\n",
              "278       0\n",
              "1         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW24ywGiGXtG"
      },
      "source": [
        "The dataset has no missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5kvJSFIHGeq",
        "outputId": "00bc2f65-1f89-40a6-94ac-41fc133b3865",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# checking for duplicates\n",
        "df.columns.duplicated()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPKaq1h_NyIo",
        "outputId": "f36f12b1-ba54-4aea-b1e6-dc9161bb5c59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['0', '0.64', '0.64.1', '0.1', '0.32', '0.2', '0.3', '0.4', '0.5', '0.6',\n",
              "       '0.7', '0.64.2', '0.8', '0.9', '0.10', '0.32.1', '0.11', '1.29', '1.93',\n",
              "       '0.12', '0.96', '0.13', '0.14', '0.15', '0.16', '0.17', '0.18', '0.19',\n",
              "       '0.20', '0.21', '0.22', '0.23', '0.24', '0.25', '0.26', '0.27', '0.28',\n",
              "       '0.29', '0.30', '0.31', '0.32.2', '0.33', '0.34', '0.35', '0.36',\n",
              "       '0.37', '0.38', '0.39', '0.40', '0.41', '0.42', '0.778', '0.43', '0.44',\n",
              "       '3.756', '61', '278', '1'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfIP3v3wPHzs"
      },
      "source": [
        "# Creating Dependent and Independent variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqGkPqcMPQE7"
      },
      "source": [
        "# independent variables\n",
        "X = df.drop(columns=\"1\")\n",
        "\n",
        "# dependent variable\n",
        "y = df[\"1\"]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dl2F1odhO3Yp"
      },
      "source": [
        "# Splitting data into train and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29uZvQu7O9jN"
      },
      "source": [
        "# splitting into train and test data\n",
        "# I will train using 70% of the data to enhanced accuracy\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTXiIfVLP8Iz"
      },
      "source": [
        "# standardizing the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaled = pd.DataFrame(StandardScaler().fit_transform(X_train))\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J5KF-XkP8h8"
      },
      "source": [
        "# Modelling Using Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIzWC3zmQEJN",
        "outputId": "0bdf61e8-1193-498e-a0eb-271e2a27d86e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# training our model\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u-zHeK2RTli",
        "outputId": "5576bab3-b5fa-4f13-aaac-80b9fb45de60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Predicting our test predictors\n",
        "#\n",
        "predicted = classifier.predict(X_test)\n",
        "print(np.mean(predicted == y_test))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8181159420289855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbGz_X3vXQbh",
        "outputId": "c5c2e247-8f41-44fb-a963-b97543a7eb3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "# Print the Confusion Matrix and slice it into four pieces\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, predicted)\n",
        "cm\n",
        "\n",
        "print('Confusion matrix\\n\\n', cm)\n",
        "\n",
        "print('\\nTrue Positives(TP) = ', cm[0,0])\n",
        "\n",
        "print('\\nTrue Negatives(TN) = ', cm[1,1])\n",
        "\n",
        "print('\\nFalse Positives(FP) = ', cm[0,1])\n",
        "\n",
        "print('\\nFalse Negatives(FN) = ', cm[1,0])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix\n",
            "\n",
            " [[582 221]\n",
            " [ 30 547]]\n",
            "\n",
            "True Positives(TP) =  582\n",
            "\n",
            "True Negatives(TN) =  547\n",
            "\n",
            "False Positives(FP) =  221\n",
            "\n",
            "False Negatives(FN) =  30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQyUfKhzXyag"
      },
      "source": [
        "The confusion matrix shows 582 + 547 = 1129 correct predictions and 221 + 30 = 251 incorrect predictions.\n",
        "\n",
        "In this case, we have\n",
        "\n",
        "- True Positives (Actual Positive:1 and Predict Positive:1) - 582\n",
        "- True Negatives (Actual Negative:0 and Predict Negative:0) - 547\n",
        "- False Positives (Actual Negative:0 but Predict Positive:1) - 221 (Type I error)\n",
        "- False Negatives (Actual Positive:1 but Predict Negative:0) - 30 (Type II error)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v0CdxNlXQnn",
        "outputId": "9ef27eef-9f8e-4012-eb1f-fc0baa8770da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "# visualize confusion matrix with seaborn heatmap\n",
        "\n",
        "cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n",
        "                                 index=['Predict Positive:1', 'Predict Negative:0'])\n",
        "\n",
        "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fc454c04b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ew7tDo1ZTHH"
      },
      "source": [
        "Classification Report; \n",
        "- Classification report is another way to evaluate the classification model performance. It displays the precision, recall, f1 and support scores for the model.\n",
        "\n",
        "We can print a classification report as follows:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4OGwKtdXQ0C",
        "outputId": "78516615-1e1b-4fe7-fffc-c78d51d1718d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# classification report\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, predicted))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.72      0.82       803\n",
            "           1       0.71      0.95      0.81       577\n",
            "\n",
            "    accuracy                           0.82      1380\n",
            "   macro avg       0.83      0.84      0.82      1380\n",
            "weighted avg       0.85      0.82      0.82      1380\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf0hPRaiZj_2"
      },
      "source": [
        "# classification accuracy\n",
        "TP = cm[0,0]\n",
        "TN = cm[1,1]\n",
        "FP = cm[0,1]\n",
        "FN = cm[1,0]"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrfAapNNZvXe",
        "outputId": "acb571cb-cba4-4842-cd82-eb443d17fa44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# print classification accuracy\n",
        "\n",
        "classification_accuracy = (TP + TN) / float(TP + TN + FP + FN)\n",
        "\n",
        "print('Classification accuracy : {0:0.4f}'.format(classification_accuracy))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification accuracy : 0.8181\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orMkpGnrZ2Z_",
        "outputId": "a3cb0a8c-6b60-494d-e8cc-2ca7ddfb5d65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# print classification error\n",
        "\n",
        "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
        "\n",
        "print('Classification error : {0:0.4f}'.format(classification_error))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification error : 0.1819\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bysFZhGRaKlK"
      },
      "source": [
        "Precision;\n",
        "- Precision can be defined as the percentage of correctly predicted positive outcomes out of all the predicted positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true and false positives (TP + FP).\n",
        "\n",
        "So, Precision identifies the proportion of correctly predicted positive outcome. It is more concerned with the positive class than the negative class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV3duewhaR_x",
        "outputId": "606bf29e-70a4-4879-f148-72a6790d0de8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# print precision score\n",
        "\n",
        "precision = TP / float(TP + FP)\n",
        "\n",
        "\n",
        "print('Precision : {0:0.4f}'.format(precision))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision : 0.7248\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lrWYGIxalsw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feTGKyhbam0o"
      },
      "source": [
        "Recall;\n",
        "- Recall can be defined as the percentage of correctly predicted positive outcomes out of all the actual positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true positives and false negatives (TP + FN)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4A7RT4Xaton",
        "outputId": "941f07f3-9c22-4fba-bdf9-9bc7a5b42962",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "recall = TP / float(TP + FN)\n",
        "\n",
        "print('Recall : {0:0.4f}'.format(recall))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recall : 0.9510\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKhnB036c_7o"
      },
      "source": [
        "# Repeating the above model with a different train : test ratio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIuuKn2YdM9Z"
      },
      "source": [
        "# splitting into train and test data\n",
        "# I will train using 60% of the data to enhanced accuracy\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.4, random_state=42)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYbz2-Trd_ED",
        "outputId": "293020e6-7b8e-47ef-8592-162dda8cf99f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# training our model\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train1, y_train1)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FyTFKw9d_mP",
        "outputId": "0820da7e-dfd1-423b-c134-df34b32e6942",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Predicting our test predictors\n",
        "#\n",
        "predicted = classifier.predict(X_test1)\n",
        "print(np.mean(predicted == y_test1))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8103260869565218\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py1xVodwd_px",
        "outputId": "17f66c24-ec74-4b5e-b485-100452bd1eda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "# Print the Confusion Matrix and slice it into four pieces\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm1 = confusion_matrix(y_test1, predicted)\n",
        "cm1\n",
        "\n",
        "print('Confusion matrix\\n\\n', cm1)\n",
        "\n",
        "print('\\nTrue Positives(TP) = ', cm1[0,0])\n",
        "\n",
        "print('\\nTrue Negatives(TN) = ', cm1[1,1])\n",
        "\n",
        "print('\\nFalse Positives(FP) = ', cm1[0,1])\n",
        "\n",
        "print('\\nFalse Negatives(FN) = ', cm1[1,0])"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix\n",
            "\n",
            " [[783 306]\n",
            " [ 43 708]]\n",
            "\n",
            "True Positives(TP) =  783\n",
            "\n",
            "True Negatives(TN) =  708\n",
            "\n",
            "False Positives(FP) =  306\n",
            "\n",
            "False Negatives(FN) =  43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH4xGCOXeorb",
        "outputId": "e3edea3e-9571-4241-af1f-5204b5853d46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "# visualize confusion matrix with seaborn heatmap\n",
        "\n",
        "cm_matrix = pd.DataFrame(data=cm1, columns=['Actual Positive:1', 'Actual Negative:0'], \n",
        "                                 index=['Predict Positive:1', 'Predict Negative:0'])\n",
        "\n",
        "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fc454ab4c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_CVmZXhe444"
      },
      "source": [
        "The confusion matrix shows 783 + 708 = 1419 correct predictions and 306 + 43 = 349 incorrect predictions.\n",
        "\n",
        "In this case, we have\n",
        "\n",
        "True Positives (Actual Positive:1 and Predict Positive:1) - 783\n",
        "True Negatives (Actual Negative:0 and Predict Negative:0) - 708\n",
        "False Positives (Actual Negative:0 but Predict Positive:1) - 306 (Type I error)\n",
        "False Negatives (Actual Positive:1 but Predict Negative:0) - 43 (Type II error)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dSLw9VmepEW",
        "outputId": "38285024-debe-4506-b264-676678b2ab03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# classification report\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test1, predicted))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.72      0.82      1089\n",
            "           1       0.70      0.94      0.80       751\n",
            "\n",
            "    accuracy                           0.81      1840\n",
            "   macro avg       0.82      0.83      0.81      1840\n",
            "weighted avg       0.85      0.81      0.81      1840\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ3cSUf4hFrM"
      },
      "source": [
        "f1-score\n",
        "- f1-score is the weighted harmonic mean of precision and recall. The best possible f1-score would be 1.0 and the worst would be 0.0. f1-score is the harmonic mean of precision and recall. So, f1-score is always lower than accuracy measures as they embed precision and recall into their computation. The weighted average of f1-score should be used to compare classifier models, not global accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxxEObrjepIP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBxOaSMlhpkl"
      },
      "source": [
        "# Recommendations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBv249dRwhP8"
      },
      "source": [
        "The best model is the one with a split of 70-30 \n",
        "- it has an accuracy of 82% and the model classifies 95.1% of the spam messages correctly(model recall)\n",
        "\n",
        "To improve on accuracy; We should apply the same reasoning using support vector machine model with the gaussian kernel.\n",
        "\n",
        "We train different models changing the regularization parameter C and then\n",
        "evaluate the accuracy, recall and precision of the model with the test set."
      ]
    }
  ]
}